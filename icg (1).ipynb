{"cells":[{"cell_type":"markdown","metadata":{},"source":["Image Caption Generation"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1853,"status":"ok","timestamp":1701873343137,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"RuKKP_AyutwD"},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.utils import to_categorical, plot_model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"]},{"cell_type":"markdown","metadata":{},"source":["Base_Dir and working_dir is the folder where u are going to save all your project related files. "]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1701873347858,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"WAdnZZN8utwG"},"outputs":[],"source":["BASE_DIR = r'D:\\near_by_share\\mlai\\ImageC'      #folder path where zip file is being extracted\n","WORKING_DIR = r'D:\\near_by_share\\mlai\\ImageC'   #folder path where zip file is being extracted"]},{"cell_type":"markdown","metadata":{},"source":["in your project folder download the zip file which is having our dataset on which we are going to train our model.\n","\n","zip file    = flickr8k data set from kaggle website.\n","\n","it contain arounnd 8000 images with around 40000 caption.\n","\n","\n","after downloading the zip file extract it in ur project folder."]},{"cell_type":"markdown","metadata":{},"source":["using VGG16() model to train our model on it"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WgXIJUEWutwH"},"outputs":[],"source":["# load vgg16 model\n","model = VGG16()\n","# restructure the model\n","model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"]},{"cell_type":"markdown","metadata":{},"source":["extracting the important feature from all the images and storing it in features ditionary from the future use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNbxJ3FgutwI"},"outputs":[],"source":["## extract features from image\n","features = {}\n","directory = os.path.join(BASE_DIR, 'Images')\n","\n","for img_name in tqdm(os.listdir(directory),desc='feature are extracting'):\n","    # load the image from file\n","    img_path = directory + '/' + img_name\n","    try:\n","      image = load_img(img_path, target_size=(224, 224))\n","    except UnidentifiedImageError as e:\n","      continue\n","    # convert image pixels to numpy array\n","    image = img_to_array(image)\n","    # reshape data for model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # preprocess image for vgg\n","    image = preprocess_input(image)\n","    # extract features\n","    feature = model.predict(image, verbose=0)\n","    # get image ID\n","    image_id = img_name.split('.')[0]\n","    # store feature\n","    features[image_id] = feature\n"]},{"cell_type":"markdown","metadata":{},"source":["we are dumping our features dictionary in pickel format. so that we dont have to extract features from all  the images each time we execute our code after closing our code. Dumping help us to directly excess the features ditionary without waiting for so long as extracting images is a time consuming process(depending upon your hardware as in my case it takes around 2 hours which do not have egpu). so prefer to go for google collab. if u dont have egpu in ur laptop. As google collab is a virtual platform  which provide us around 12gb of ram and 15gb of egpu which makes our extracting time from 2hr to 12 min."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXqKMcgPutwI"},"outputs":[],"source":["# pickle.dump(features, open(os.path.join(WORKING_DIR, 'features (1).pkl'), 'wb'))"]},{"cell_type":"markdown","metadata":{},"source":["and after dumping u can load the features (1).pkl file in ur code easily"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4528,"status":"ok","timestamp":1701873358824,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"YKhacCWButwI"},"outputs":[],"source":["# load features from pickle\n","with open(os.path.join(WORKING_DIR, 'features (1).pkl'), 'rb') as f:\n","    features = pickle.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["ur extracted zip file is going to contain the captions.txt file"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":940,"status":"ok","timestamp":1701873361406,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"5x0GHN25utwJ"},"outputs":[],"source":["with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n","    next(f)\n","    captions_doc = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["50817c991fea4d80abb2a910ac060fbf","c4ea2f860dc74d65b6daa5eddafe6dd6","25beff78c65142cc8791f7772a3a2f1f","53f499cd17ac4b1185eb0b988e0e5a02","1e4dca547f754f1094ce77c1e3680791","e09df816b63d47728733df541db9cfc7","4362b72c5b264ca39acb8d6244f743da","aa7b488b2b0a4fbb91b10192d55b3cf3","ee17762ca82b49f3ac58f77674ef8689","ac1df00099ce4be6b9628c4cb35155a0","985d1d40fad349f18289b896816cf503"]},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1701870585266,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"qb8t81F_utwJ","outputId":"8abd61a8-8ba4-4fbc-d072-2bf6cf87d1ec"},"outputs":[],"source":["# create mapping of image to captions\n","mapping = {}\n","# process lines\n","for line in tqdm(captions_doc.split('\\n')):\n","    # split the line by comma(,)\n","    tokens = line.split(',')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    # remove extension from image ID\n","    image_id = image_id.split('.')[0]\n","    # convert caption list to string\n","    caption = \" \".join(caption)\n","    # create list if needed\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    # store the caption\n","    mapping[image_id].append(caption)"]},{"cell_type":"markdown","metadata":{},"source":["clean mapping function remove unwanted character from the captions which can create problem for our machine to learn."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":585,"status":"ok","timestamp":1701870592308,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"dju9CNmmutwJ"},"outputs":[],"source":["def clean(mapping):\n","    for key, captions in mapping.items():\n","        for i in range(len(captions)):\n","            # take one caption at a time\n","            caption = captions[i]\n","            # preprocessing steps\n","            # convert to lowercase\n","            caption = caption.lower()\n","            # delete digits, special chars, etc.,\n","            caption = caption.replace('[^A-Za-z]', '')\n","            # delete additional spaces\n","            caption = caption.replace('\\s+', ' ')\n","            # add start and end tags to the caption\n","            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n","            captions[i] = caption"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701870594616,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"yOCSlN4nutwK","outputId":"2716bc45-1a9b-457b-c633-bc6628a73597"},"outputs":[],"source":["# before preprocess of text\n","mapping['17273391_55cfc7d3d4']"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1701870601906,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"5ofzVMI7utwK"},"outputs":[],"source":["# preprocess the text\n","clean(mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1701870606498,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"nYGHZ3bButwK","outputId":"d7099dd9-17f1-446c-b575-edf27e11e0a7"},"outputs":[],"source":["# after preprocess of text\n","mapping['17273391_55cfc7d3d4']"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":631,"status":"ok","timestamp":1701870613354,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"QjiESu9RutwK"},"outputs":[],"source":["all_captions = []\n","for key in mapping:\n","    for caption in mapping[key]:\n","        all_captions.append(caption)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1483,"status":"ok","timestamp":1701870619050,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"m2aUQlj6utwL"},"outputs":[],"source":["# tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_captions)\n","vocab_size = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701870622023,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"e_QmVQF2Vo42"},"outputs":[],"source":["# Save the tokenizer and max length for later use during prediction\n","with open(os.path.join(WORKING_DIR, 'tokenizer.pkl'), 'wb') as token_file:\n","    pickle.dump(tokenizer, token_file)"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":492,"status":"ok","timestamp":1701869324717,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"v_kUwwKn7Q_p"},"outputs":[],"source":["with open(os.path.join(WORKING_DIR, 'tokenizer.pkl'), 'rb') as token_file:\n","        tokenizer = pickle.load(token_file)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":550,"status":"ok","timestamp":1701870627890,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"UhOIz3zJutwL","outputId":"1b2b2155-e8fd-4d6e-dd08-d77d089d1df6"},"outputs":[{"data":{"text/plain":["35"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["#  get maximum length of the caption available\n","max_length = max(len(caption.split()) for caption in all_captions)\n","max_length"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701870631476,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"dlCaOR4CWPtc"},"outputs":[],"source":["with open(os.path.join(WORKING_DIR, 'max_length.pkl'), 'wb') as maxlen_file:\n","    pickle.dump(max_length, maxlen_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":715,"status":"ok","timestamp":1701873385601,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"HIBILEjZ8XF3"},"outputs":[],"source":["with open(os.path.join(WORKING_DIR, 'max_length.pkl'), 'rb') as maxlen_file:\n","        max_length = pickle.load(maxlen_file)"]},{"cell_type":"markdown","metadata":{},"source":["use 90% of images in flickr8k dataset for training the model and rest for testing\n","\n","can be commented after  u train ur model and dump ur model as after dumping the trained model u dont have to train it again and again "]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1701870638379,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"1ED7j6J5utwM"},"outputs":[],"source":["image_ids = list(mapping.keys())\n","split = int(len(image_ids) * 0.90)\n","train = image_ids[:split]\n","test = image_ids[split:]"]},{"cell_type":"markdown","metadata":{},"source":["can be commented after dumping our trained model"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":764,"status":"ok","timestamp":1701870643826,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"zo9Va0y1utwM"},"outputs":[],"source":["# create data generator to get data in batch (avoids session crash)\n","def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n","    # loop over images\n","    X1, X2, y = list(), list(), list()\n","    n = 0\n","    while 1:\n","        for key in data_keys:\n","            n += 1\n","            captions = mapping[key]\n","            # process each caption\n","            for caption in captions:\n","                # encode the sequence\n","                seq = tokenizer.texts_to_sequences([caption])[0]\n","                # split the sequence into X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pairs\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\n","                    # store the sequences\n","                    X1.append(features[key][0])\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            if n == batch_size:\n","                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","                yield [X1, X2], y\n","                X1, X2, y = list(), list(), list()\n","                n = 0"]},{"cell_type":"markdown","metadata":{},"source":["add various layer criteria for the model"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":963,"status":"ok","timestamp":1701870657212,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"BUOivsDwutwM"},"outputs":[],"source":["# encoder model\n","# image feature layers\n","inputs1 = Input(shape=(4096,))\n","fe1 = Dropout(0.4)(inputs1)\n","fe2 = Dense(256, activation='relu')(fe1)\n","# sequence feature layers\n","inputs2 = Input(shape=(max_length,))\n","se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","se2 = Dropout(0.4)(se1)\n","se3 = LSTM(256)(se2)\n","\n","# decoder model\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# plot the model\n","plot_model(model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["this will also gonna take time according to ur hardware specification\n","\n","\n","here we are training our model over 90% of 8091 images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KNkJd0yutwM"},"outputs":[],"source":["# train the model\n","epochs = 15\n","batch_size = 32\n","steps = len(train) // batch_size\n","\n","for i in range(epochs):\n","    # create data generator\n","    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n","    # fit for one epoch\n","    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"CKZ7dalDuNUK"},"outputs":[],"source":["# model.save('D:\\near_by_share\\mlai\\ImageC\\modeltrain1.h5')"]},{"cell_type":"markdown","metadata":{},"source":["after saving our model we can also comment the training part as model has been trained and we can simply load it in any of our program  "]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4080,"status":"ok","timestamp":1701871784025,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"H0r_zxPFTFr3"},"outputs":[],"source":["from keras.models import load_model\n","model = load_model(r'D:\\near_by_share\\mlai\\ImageC\\modeltrain1.h5')"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701873390985,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"_lYJ336JutwM"},"outputs":[],"source":["def idx_to_word(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":865,"status":"ok","timestamp":1701873395479,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"ITcp7Pa2utwM"},"outputs":[],"source":["# generate caption for an image\n","def predict_caption(model, image, tokenizer, max_length):\n","    # add start tag for generation process\n","    in_text = 'startseq'\n","    # iterate over the max length of sequence\n","    for i in range(max_length):\n","        # encode input sequence\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        # pad the sequence\n","        sequence = pad_sequences([sequence], max_length)\n","        # predict next word\n","        yhat = model.predict([image, sequence], verbose=0)\n","        # get index with high probability\n","        yhat = np.argmax(yhat)\n","        # convert index to word\n","        word = idx_to_word(yhat, tokenizer)\n","        # stop if word not found\n","        if word is None:\n","            break\n","        # append word as input for generating next word\n","        in_text += \" \" + word\n","        # stop if we reach end tag\n","        if word == 'endseq':\n","            break\n","\n","    return in_text"]},{"cell_type":"markdown","metadata":{},"source":["accuracy of our program can be test on the remaining test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcEp8KsmutwM"},"outputs":[],"source":["# from nltk.translate.bleu_score import corpus_bleu\n","# # validate with test data\n","# actual, predicted = list(), list()\n","\n","# for key in tqdm(test):\n","#     # get actual caption\n","#     captions = mapping[key]\n","#     # predict the caption for image\n","#     y_pred = predict_caption(model, features, tokenizer, max_length)\n","#     # split into words\n","#     actual_captions = [caption.split() for caption in captions]\n","#     y_pred = y_pred.split()\n","#     # append to the list\n","#     actual.append(actual_captions)\n","#     predicted.append(y_pred)\n","\n","# # calcuate BLEU score\n","# print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","# print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"]},{"cell_type":"markdown","metadata":{},"source":["check your model over different images"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701871796745,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"b66dMYveZxXU"},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","\n","class CaptionGenerator:\n","    def __init__(self):\n","        # Initialize your class if needed\n","        pass\n","\n","    def generate_caption(self, image_name):\n","        image_id = image_name.split('.')[0]\n","        img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n","        image = Image.open(img_path)\n","        captions = mapping[image_id]\n","        print('---------------------Actual---------------------')\n","        for caption in captions:\n","            print(caption)\n","        # predict the caption\n","        y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n","        print('--------------------Predicted--------------------')\n","        print(y_pred)\n","        plt.imshow(image)\n","\n","# Create an instance of the CaptionGenerator class\n","models = CaptionGenerator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntoY8hilbJU9"},"outputs":[],"source":["models.generate_caption(\"230016181_0c52b95304.jpg\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["run ur predict_caption func for any image of ur choice"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5043,"status":"ok","timestamp":1701873414673,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"IITNFJ_yutwN","outputId":"417059fb-f725-4203-a240-96a517e3e5f3"},"outputs":[],"source":["vgg_model = VGG16()\n","# restructure the model\n","vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":57252,"status":"ok","timestamp":1701873478319,"user":{"displayName":"Kunal Mehra","userId":"01298929674880346817"},"user_tz":-330},"id":"3d5MizKjutwN","outputId":"4ad0e975-cbc9-4ccc-80a8-19b6d3e8e81f"},"outputs":[{"data":{"text/plain":["'startseq man in yellow shirt and black pants is performing trick on his bicycle endseq'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["image_path = 'D:/near_by_share/mlai/ImageC/Images/47870024_73a4481f7d.jpg'\n","# load image\n","image = load_img(image_path, target_size=(224, 224))\n","# convert image pixels to numpy array\n","image = img_to_array(image)\n","# reshape data for model\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","# preprocess image for vgg\n","image = preprocess_input(image)\n","# extract features\n","feature = vgg_model.predict(image, verbose=0)\n","# predict from the trained model\n","predict_caption(model, feature, tokenizer, max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJn21NZuutwO"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["If u want to make ur model more interactive u can go for flask and html process repository in my repository section"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"1e4dca547f754f1094ce77c1e3680791":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25beff78c65142cc8791f7772a3a2f1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa7b488b2b0a4fbb91b10192d55b3cf3","max":40456,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee17762ca82b49f3ac58f77674ef8689","value":40456}},"4362b72c5b264ca39acb8d6244f743da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50817c991fea4d80abb2a910ac060fbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4ea2f860dc74d65b6daa5eddafe6dd6","IPY_MODEL_25beff78c65142cc8791f7772a3a2f1f","IPY_MODEL_53f499cd17ac4b1185eb0b988e0e5a02"],"layout":"IPY_MODEL_1e4dca547f754f1094ce77c1e3680791"}},"53f499cd17ac4b1185eb0b988e0e5a02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac1df00099ce4be6b9628c4cb35155a0","placeholder":"​","style":"IPY_MODEL_985d1d40fad349f18289b896816cf503","value":" 40456/40456 [00:00&lt;00:00, 376088.25it/s]"}},"985d1d40fad349f18289b896816cf503":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa7b488b2b0a4fbb91b10192d55b3cf3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac1df00099ce4be6b9628c4cb35155a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4ea2f860dc74d65b6daa5eddafe6dd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e09df816b63d47728733df541db9cfc7","placeholder":"​","style":"IPY_MODEL_4362b72c5b264ca39acb8d6244f743da","value":"100%"}},"e09df816b63d47728733df541db9cfc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee17762ca82b49f3ac58f77674ef8689":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
